{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPuaSmnp9qZJNhlUEX59gNK"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Environment Setup"
      ],
      "metadata": {
        "id": "nqa3iqrmFfCm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Library Installation"
      ],
      "metadata": {
        "id": "G-nVqny4FnIw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JNY8By6FXni",
        "outputId": "77f359a1-af26-4080-f12c-603dd66326fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n",
            "E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/universe/o/openjdk-8/openjdk-8-jre-headless_8u462-ga%7eus1-0ubuntu2%7e22.04.2_amd64.deb  404  Not Found [IP: 185.125.190.39 80]\n",
            "E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/universe/o/openjdk-8/openjdk-8-jdk-headless_8u462-ga%7eus1-0ubuntu2%7e22.04.2_amd64.deb  404  Not Found [IP: 185.125.190.39 80]\n",
            "E: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Library Import"
      ],
      "metadata": {
        "id": "s7bBbnGzFpJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, to_timestamp, year\n",
        "import time"
      ],
      "metadata": {
        "id": "IqeNjUwEFlc1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading and Preprocessing"
      ],
      "metadata": {
        "id": "gxgo_UdUF0Nl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kaggle"
      ],
      "metadata": {
        "id": "RVpwmKiZF2Q7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZ8rcGJyF7f2",
        "outputId": "bd6b61c0-60b7-421e-d092-a8fb87b0c2f2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle"
      ],
      "metadata": {
        "id": "jIOf1xccF9ad"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/CollabData/kaggle_API/kaggle.json ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "7JDLwI7YGAU9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "YR2_gmTeGB_X"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download har5hdeep5harma/chicago-crime-incidents-2001-to-present"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prFSfbbHGDeZ",
        "outputId": "fdf4f15d-a94d-452d-f03a-8b909fc2faec"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/har5hdeep5harma/chicago-crime-incidents-2001-to-present\n",
            "License(s): other\n",
            "Downloading chicago-crime-incidents-2001-to-present.zip to /content\n",
            " 98% 508M/516M [00:03<00:00, 164MB/s]\n",
            "100% 516M/516M [00:03<00:00, 179MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip chicago-crime-incidents-2001-to-present.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boSk6SUyGEpC",
        "outputId": "ffcd9d98-2328-4f3b-844d-91020e9ad0b7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  chicago-crime-incidents-2001-to-present.zip\n",
            "  inflating: Chicago_Crimes_2001_to_Present.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize Spark Session"
      ],
      "metadata": {
        "id": "hkv6_WveGLr-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Updating apt-get and installing openjdk-8-jdk-headless...\")\n",
        "!apt-get update -qq > /dev/null\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "# These imports are also in IqeNjUwEFlc1, but included here for robustness if cell is run in isolation\n",
        "from pyspark.sql.functions import col, to_timestamp, year\n",
        "import time\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"StorageTechniques_ChicagoCrime\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"Spark initialized\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hrG8DhzGN8W",
        "outputId": "588dedea-9152-49fe-c79b-2b98afcba2e0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updating apt-get and installing openjdk-8-jdk-headless...\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Spark initialized\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and Prepare Data"
      ],
      "metadata": {
        "id": "1E-N_ofpGRYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the raw CSV\n",
        "print(\"Loading CSV data...\")\n",
        "df_raw = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"Chicago_Crimes_2001_to_Present.csv\")\n",
        "\n",
        "# Clean up column names by removing spaces\n",
        "new_columns = [c.replace(' ', '_') for c in df_raw.columns]\n",
        "df = df_raw.toDF(*new_columns)\n",
        "\n",
        "# Ensure Date format and Year column exist (used for partitioning)\n",
        "df = df.withColumn(\"Year\", col(\"Year\").cast(\"integer\"))\n",
        "\n",
        "print(f\"Total rows: {df.count()}\")\n",
        "df.printSchema()\n",
        "df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlnx0adyGS7i",
        "outputId": "7dc97d58-da36-45e5-85d9-2734d8d37cb8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading CSV data...\n",
            "Total rows: 8422096\n",
            "root\n",
            " |-- ID: integer (nullable = true)\n",
            " |-- Case_Number: string (nullable = true)\n",
            " |-- Date: string (nullable = true)\n",
            " |-- Block: string (nullable = true)\n",
            " |-- IUCR: string (nullable = true)\n",
            " |-- Primary_Type: string (nullable = true)\n",
            " |-- Description: string (nullable = true)\n",
            " |-- Location_Description: string (nullable = true)\n",
            " |-- Arrest: boolean (nullable = true)\n",
            " |-- Domestic: boolean (nullable = true)\n",
            " |-- Beat: integer (nullable = true)\n",
            " |-- District: integer (nullable = true)\n",
            " |-- Ward: integer (nullable = true)\n",
            " |-- Community_Area: integer (nullable = true)\n",
            " |-- FBI_Code: string (nullable = true)\n",
            " |-- X_Coordinate: integer (nullable = true)\n",
            " |-- Y_Coordinate: integer (nullable = true)\n",
            " |-- Year: integer (nullable = true)\n",
            " |-- Updated_On: string (nullable = true)\n",
            " |-- Latitude: double (nullable = true)\n",
            " |-- Longitude: double (nullable = true)\n",
            " |-- Location: string (nullable = true)\n",
            "\n",
            "+--------+-----------+--------------------+--------------------+----+--------------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+------------+-------------+--------------------+\n",
            "|      ID|Case_Number|                Date|               Block|IUCR|        Primary_Type|         Description|Location_Description|Arrest|Domestic|Beat|District|Ward|Community_Area|FBI_Code|X_Coordinate|Y_Coordinate|Year|          Updated_On|    Latitude|    Longitude|            Location|\n",
            "+--------+-----------+--------------------+--------------------+----+--------------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+------------+-------------+--------------------+\n",
            "|13311263|   JG503434|07/29/2022 03:39:...|     023XX S TROY ST|1582|OFFENSE INVOLVING...|   CHILD PORNOGRAPHY|           RESIDENCE|  true|   false|1033|      10|  25|            30|      17|        NULL|        NULL|2022|04/18/2024 03:40:...|        NULL|         NULL|                NULL|\n",
            "|13053066|   JG103252|01/03/2023 04:44:...|039XX W WASHINGTO...|2017|           NARCOTICS|MANUFACTURE / DEL...|            SIDEWALK|  true|   false|1122|      11|  28|            26|      18|        NULL|        NULL|2023|01/20/2024 03:41:...|        NULL|         NULL|                NULL|\n",
            "|12131221|   JD327000|08/10/2020 09:45:...|   015XX N DAMEN AVE|0326|             ROBBERY|AGGRAVATED VEHICU...|              STREET|  true|   false|1424|      14|   1|            24|      03|     1162795|     1909900|2020|05/17/2025 03:40:...|41.908417822| -87.67740693|(41.908417822, -8...|\n",
            "|11227634|   JB147599|08/26/2017 10:00:...| 001XX W RANDOLPH ST|0281| CRIM SEXUAL ASSAULT|      NON-AGGRAVATED|         HOTEL/MOTEL| false|   false| 122|       1|  42|            32|      02|        NULL|        NULL|2017|02/11/2018 03:57:...|        NULL|         NULL|                NULL|\n",
            "|13203321|   JG415333|09/06/2023 05:00:...|    002XX N Wells st|1320|     CRIMINAL DAMAGE|          TO VEHICLE|PARKING LOT / GAR...| false|   false| 122|       1|  42|            32|      14|     1174694|     1901831|2023|11/04/2023 03:40:...|41.886018055|-87.633937881|(41.886018055, -8...|\n",
            "+--------+-----------+--------------------+--------------------+----+--------------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+------------+-------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Storage Comparison"
      ],
      "metadata": {
        "id": "NQf2ezQBGYQL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### .csv"
      ],
      "metadata": {
        "id": "oo-DGl_bGcV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save as CSV\n",
        "print(\"Writing CSV...\")\n",
        "start_time = time.time()\n",
        "df.write.mode(\"overwrite\").csv(\"chicago_data_csv\")\n",
        "print(f\"CSV Write Time: {time.time() - start_time:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZj5N_HKGaxd",
        "outputId": "e1a1c80d-107f-4de0-e1d6-6b2de3aee1a4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing CSV...\n",
            "CSV Write Time: 113.33 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### parquet"
      ],
      "metadata": {
        "id": "NbttknkCGfoa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save as Parquet\n",
        "print(\"Writing Parquet...\")\n",
        "start_time = time.time()\n",
        "df.write.mode(\"overwrite\").parquet(\"chicago_data_parquet\")\n",
        "print(f\"Parquet Write Time: {time.time() - start_time:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhRlN3s0GhVK",
        "outputId": "a238b1a9-00a4-4448-9647-cd0af4f53c84"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing Parquet...\n",
            "Parquet Write Time: 120.13 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Storage Comparison"
      ],
      "metadata": {
        "id": "hsn9E4HLGloW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare Storage Size\n",
        "print(\"\\nSTORAGE SIZE COMPARISON\")\n",
        "!du -sh chicago_data_csv\n",
        "!du -sh chicago_data_parquet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpztB5gHGn0H",
        "outputId": "934aa753-ed14-4efb-f83f-d39e70b769d1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "STORAGE SIZE COMPARISON\n",
            "1.9G\tchicago_data_csv\n",
            "568M\tchicago_data_parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Partitioning Strategy"
      ],
      "metadata": {
        "id": "VJ3F5rlcG40E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write Partitioned Parquet\n",
        "print(\"Writing Partitioned Parquet (partitioned by Year)...\")\n",
        "start_time = time.time()\n",
        "df.write.mode(\"overwrite\").partitionBy(\"Year\").parquet(\"chicago_data_partitioned\")\n",
        "print(f\"Partitioned Write Time: {time.time() - start_time:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgq1J23QG8Kd",
        "outputId": "0a7f2ba5-c229-4a79-e48f-cc6bb1ccddce"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing Partitioned Parquet (partitioned by Year)...\n",
            "Partitioned Write Time: 148.32 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show folder structure\n",
        "print(\"\\nPARTITION STRUCTURE\")\n",
        "!ls -F chicago_data_partitioned | head -n 15"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHBObGWaG_1M",
        "outputId": "303e813c-6830-46fa-ecf8-6896c682106f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "PARTITION STRUCTURE\n",
            "_SUCCESS\n",
            "Year=2001/\n",
            "Year=2002/\n",
            "Year=2003/\n",
            "Year=2004/\n",
            "Year=2005/\n",
            "Year=2006/\n",
            "Year=2007/\n",
            "Year=2008/\n",
            "Year=2009/\n",
            "Year=2010/\n",
            "Year=2011/\n",
            "Year=2012/\n",
            "Year=2013/\n",
            "Year=2014/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query Performance Benchmarking"
      ],
      "metadata": {
        "id": "vzDoO3ICHCnX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_year = 2024\n",
        "\n",
        "# Query 1: Non-Partitioned\n",
        "# Spark must scan the entire dataset to find rows where Year = 2024\n",
        "print(f\"Querying Non-Partitioned Data for Year {target_year}...\")\n",
        "start = time.time()\n",
        "count_raw = spark.read.parquet(\"chicago_data_parquet\").filter(f\"Year = {target_year}\").count()\n",
        "print(f\"Time: {time.time() - start:.4f} seconds (Count: {count_raw})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNZGfE9FHFnE",
        "outputId": "e30e23d3-364c-4bf1-8275-8479c67217fb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Querying Non-Partitioned Data for Year 2024...\n",
            "Time: 3.4280 seconds (Count: 258734)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Query 2: Partitioned\n",
        "# Spark goes directly to the folder /Year=2024/ and reads only that data\n",
        "print(f\"Querying Partitioned Data for Year {target_year}...\")\n",
        "start = time.time()\n",
        "count_part = spark.read.parquet(\"chicago_data_partitioned\").filter(f\"Year = {target_year}\").count()\n",
        "print(f\"Time: {time.time() - start:.4f} seconds (Count: {count_part})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3fUDUCVHMW0",
        "outputId": "213c1c6a-34be-4735-9e05-8a1e3cd77d18"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Querying Partitioned Data for Year 2024...\n",
            "Time: 0.7997 seconds (Count: 258734)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compression and Download"
      ],
      "metadata": {
        "id": "7W26pRp4HS3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Compressing CSV output...\")\n",
        "!tar -czf chicago_data_csv.tar.gz chicago_data_csv\n",
        "\n",
        "print(\"Compressing Parquet output...\")\n",
        "!tar -czf chicago_data_parquet.tar.gz chicago_data_parquet\n",
        "\n",
        "print(\"Files ready for download:\")\n",
        "!ls -lh *.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Grfh2K1HYWg",
        "outputId": "db2227ee-8230-4096-e401-230a29139383"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compressing CSV output...\n",
            "Compressing Parquet output...\n",
            "Files ready for download:\n",
            "-rw-r--r-- 1 root root 515M Nov 26 03:16 chicago_data_csv.tar.gz\n",
            "-rw-r--r-- 1 root root 457M Nov 26 03:18 chicago_data_parquet.tar.gz\n"
          ]
        }
      ]
    }
  ]
}